{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtrPYNGk1HIWquY60bTo4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuwidzz/sentimentanalysis/blob/main/sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h8ZaeRpeyo5V",
        "outputId": "7a47de05-d1b5-49ce-f53f-7ebd465c8252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.17.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Hyperparameter configuration\n",
        "EPOCH = 50  # Reduce epochs to prevent overfitting\n",
        "BATCH = 32\n",
        "LEARNING_RATE = 2e-5  # Lower learning rate for fine-tuning\n",
        "MODEL_PATH = \"models/my-albert-202501031441.h5\"  # Path to save model weights\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Baca dataset dari CSV\n",
        "file_path = 'dataset/pilkada_sentiment_dataset.csv'\n",
        "df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "\n",
        "# Persiapkan data\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['text'].values,\n",
        "    df['sentiment'].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Unduh ALBERT Pre-trained Model\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "model = TFAlbertForSequenceClassification.from_pretrained('albert-base-v2')\n",
        "\n",
        "# Preprocessing Data\n",
        "max_length = 500\n",
        "\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "# Ekstrak Array NumPy\n",
        "train_input_ids = train_encodings['input_ids'].numpy()\n",
        "train_attention_mask = train_encodings['attention_mask'].numpy()\n",
        "\n",
        "test_input_ids = test_encodings['input_ids'].numpy()\n",
        "test_attention_mask = test_encodings['attention_mask'].numpy()\n",
        "\n",
        "# Konversi label sentimen menjadi bentuk numerik\n",
        "label_mapping = {\n",
        "    'very positive': 0,\n",
        "    'positive': 1,\n",
        "    'neutral': 2,\n",
        "    'negative': 3,\n",
        "    'very negative': 4\n",
        "}\n",
        "train_labels_numeric = [label_mapping.get(label, 0) for label in train_labels]\n",
        "test_labels_numeric = [label_mapping.get(label, 0) for label in test_labels]\n",
        "\n",
        "# Buat tf.data.Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(((train_input_ids, train_attention_mask), train_labels_numeric))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(((test_input_ids, test_attention_mask), test_labels_numeric))\n",
        "\n",
        "# Training Model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_dataset.batch(BATCH), epochs=EPOCH)\n",
        "\n",
        "# Evaluasi Model\n",
        "eval_results = model.evaluate(test_dataset.batch(BATCH))\n",
        "print(\"Test loss:\", eval_results[0])\n",
        "print(\"Test accuracy:\", eval_results[1])\n",
        "\n",
        "# Prediksi dengan Model yang Telah Dilatih\n",
        "new_texts = ['Bangga sekali Calon ini bisa Merangkul', 'Keren Sekali Visi Misinya', 'Hebat Sekali Kerjanya']\n",
        "new_encodings = tokenizer(new_texts, truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "new_input_ids = new_encodings['input_ids'].numpy()\n",
        "new_attention_mask = new_encodings['attention_mask'].numpy()\n",
        "\n",
        "predictions = model.predict([new_input_ids, new_attention_mask])\n",
        "logits = predictions.logits\n",
        "predicted_labels = tf.argmax(logits, axis=1).numpy()\n",
        "predicted_sentiments = [list(label_mapping.keys())[list(label_mapping.values()).index(label)] for label in predicted_labels]\n",
        "print(\"Predicted sentiments:\", predicted_sentiments)\n",
        "\n",
        "# Simpan model di folder models\n",
        "model.save_weights(MODEL_PATH)\n",
        "print(f\"Model weights saved to {MODEL_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wxXudYw5bPA",
        "outputId": "c24de783-1767-46d2-89a3-277fff6a9829"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFAlbertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFAlbertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - 17s 183ms/step - loss: nan - accuracy: 0.2058\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 2s 199ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - 2s 203ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - 2s 200ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - 2s 202ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - 2s 182ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - 2s 181ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - 2s 196ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - 2s 184ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - 2s 184ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - 2s 181ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - 2s 184ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - 2s 194ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - 2s 187ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - 2s 188ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - 2s 195ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - 2s 197ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - 3s 212ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - 3s 205ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - 2s 191ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - 2s 190ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - 2s 186ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - 2s 193ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - 2s 178ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - 2s 186ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - 2s 191ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - 2s 179ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - 2s 180ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - 2s 188ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - 2s 191ms/step - loss: nan - accuracy: 0.2137\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - 2s 181ms/step - loss: nan - accuracy: 0.2137\n",
            "3/3 [==============================] - 3s 111ms/step - loss: nan - accuracy: 0.2316\n",
            "Test loss: nan\n",
            "Test accuracy: 0.23157894611358643\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "Predicted sentiments: ['very positive', 'very positive', 'very positive']\n",
            "Model weights saved to models/my-albert-202501031441.h5\n"
          ]
        }
      ]
    }
  ]
}